{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Machine Learning: Assignment 1\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 1 is due Wednesday, September 28 11:59pm. Late work will not be accepted as per the course policies (see the syllabus on Canvas).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:\n",
    "\n",
    "Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "Under \"Download as\", select \"HTML (.html)\"\n",
    "After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "\n",
    " * Lasso\n",
    " * Bias-variance decomposition\n",
    " * Mercer kernels\n",
    " * Risk properties of neural networks\n",
    "\n",
    "This assignment will also help to solidify your Python and Jupyter notebook skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Roping variables with the Lasso (15 points)\n",
    "\n",
    "In this problem, you will use the lasso \n",
    "to round up some predictor variables. \n",
    "We have prepared a data set with $y = X\\beta + \\sigma \\epsilon$ where $\\beta$ is a sparse vector and $\\epsilon_i \\sim N(0,1)$ is Gaussian noise. Your task is three-fold:\n",
    "\n",
    "* Generate a plot of the lasso regularization paths;\n",
    "* Determine which coefficients of $\\beta$ are nonzero;\n",
    "* Give your best estimate of these nonzero coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.linear_model import Lasso\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just run the next cell to read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = pd.read_pickle('https://raw.githubusercontent.com/YData123/sds365-fa22/main/assignments/assn1/problem1_Xy.pkl')\n",
    "n, p = X.shape\n",
    "print(\"Number of rows: {}\".format(n))\n",
    "print(\"Number of columns: {}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Lasso regularization paths\n",
    "\n",
    "Run the lasso and plot the regularization paths. You can use the `Lasso` class from the `sklearn.linear_model` package. Plot the parameter paths with the regularization level $\\lambda$ (`alpha` in the code) on the log-scale, as done in the lasso demo code from class. (As always, be sure to label your axes.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Select, estimate, and predict \n",
    "\n",
    "The true model is linear, and only a subset $S \\subset \\{0,1,\\ldots, 49\\}$ of the 50 variables have non-zero coefficients $\\beta_j$. In this problem you should make three estimates: \n",
    "\n",
    "1. An estimate $\\hat S$ of $S$\n",
    "2. An estimate $\\hat \\beta_j$ for each $j\\in \\hat S$\n",
    "3. An estimate of the predictive risk ${\\mathbb E}(Y - X\\hat\\beta)$\n",
    "\n",
    "\n",
    "We are not specifying how you should construct these estimates. You should use your judgement, taste, and \n",
    "the tools provided from class. However, you must clearly explain and justify whatever approach that you use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (and Markdown if needed) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Risky business (10 points)\n",
    "\n",
    "In class [(and in these notes)](https://github.com/YData123/sds365-fa22/raw/main/notes/kernel-bias-variance.pdf) we sketched a proof that, when the regression function has two bounded derivatives,\n",
    " the bias and variance for kernel smoothing scale as\n",
    "\n",
    "$$ \\mbox{bias}^2 = O\\left(h^4\\right)$$\n",
    "$$ \\mbox{var} = O\\left(\\frac{1}{nh^p}\\right).$$\n",
    "\n",
    "Here $h$ is the bandwidth parameter, $n$ is the sample size, and $p$ is the number of predictor variables. These expressions are asymptotic, meaning that they apply as $n$ gets large and $h$ gets small.  In this problem your job is to reason about the implications of this bias-variance decomposition for prediction.\n",
    "\n",
    "*Note:* For this problem, you may either enter your answers in Markdown using $\\rm\\LaTeX$, or you write them on paper and scan to insert as an image in the notebook; whichever you prefer.\n",
    "\n",
    "\n",
    "### 2.1 Selecting the optimal bandwidth\n",
    "\n",
    "Suppose that the bias and variance are such that \n",
    "\n",
    "$$ \\mbox{bias}^2(\\hat m(x))  \\leq c_1 h^4 $$\n",
    "$$ \\mbox{var}(\\hat m(x)) \\leq c_2 \\frac{1}{nh^p}.$$\n",
    "\n",
    "for two constants $c_1$ and $c_2$. Using these expressions and a little calculus, determine the optimal bandwidth $h$ to minimize the risk function \n",
    "\n",
    "$$R(h) = {\\mathbb E}\\left(\\hat m(x) - m(x)\\right)^2.$$\n",
    "\n",
    "Your answer should involve the constants $c_1, c_2$, and $n$ and $p$. Give a bound on the resulting risk using this bandwidth.\n",
    "\n",
    "\n",
    "### 2.2 Bandwith selection without tears\n",
    "\n",
    "Now, going back to the expressions $\\mbox{bias}^2 = O\\left(h^4\\right)$ and $ \\mbox{var} = O\\left(\\displaystyle\\frac{1}{nh^p}\\right)$, explain why the scaling of the optimal bandwidth (as a function of $n$ and $p$), must satisfy \n",
    "$\\mbox{bias}^2  \\approx \\mbox{var}$; that is, they must be of the same order as $h\\to 0$. Then, without using any calculus, use this argument to determine the optimal scaling of the bandwidth and the fastest rate at which the \n",
    "risk $R(h) = {\\mathbb E}\\left(\\hat m(x) - m(x)\\right)^2$ can approach zero as the sample size increases.\n",
    "\n",
    "\n",
    "### 2.3 The cursed COD\n",
    "\n",
    "Using the risk bound you derive above, make a plot that demonstrates the curse of dimensionality by plotting the sample size required to achieve a given level of risk. Specifically, let the target risk $R$ vary between 0.1 and 0.5, and let the dimension $p$ vary between 1 and 20, and plot the sample size required to achieve that risk. Give a single plot that shows the collection of curves for each dimension.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and markdown with derivations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: A kernel of truth (15 points)\n",
    "\n",
    "For problem you will implement nonparametric regression using Mercer kernels and penalization, in 1-dimension. This can be compared to regression using smoothing kernels. \n",
    "\n",
    "As discussed in lecture, nonparametric regression with Mercer kernels is based on the infinite dimensional ridge regression\n",
    "\n",
    "$$ \\hat m = \\mbox{argmin} \\| Y - m \\|^2 + \\lambda \\|m\\|_K^2$$\n",
    "\n",
    "By the representer theorem, this is equivalent to setting $\\hat m(x) = \\sum_{i=1}^n \\hat \\alpha_i K(X_i, x)$ and \n",
    "using the finite dimensional optimization\n",
    "\n",
    "$$ \\hat \\alpha = \\mbox{argmin} \\| Y - {\\mathbb K} \\alpha \\|^2 + \\lambda \\alpha^T {\\mathbb K} \\alpha$$\n",
    "\n",
    "###  3.1 Solve \n",
    "\n",
    "Derive a closed-form expression for the minimizer $\\hat\\alpha$. Show all of the steps in your derivation, \n",
    "and justify each step. (As above, you may either enter your answers in Markdown using $\\rm\\LaTeX$, or insert an image of your handwritten solution.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.2 Implement\n",
    "\n",
    "Next you will use your solution above and implement Mercer kernel regression. We give some starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines some \"helper functions\" for this exercise. You don't need to change any of this code.\n",
    "(If you do want to make changes, just describe what you did and why.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_estimate(x, f, fhat, X, y, sigma, lmbda, sleeptime=.01):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(x, f, color='red', linewidth=2, label='true function')\n",
    "    plt.plot(x, fhat, color='blue', linewidth=2, label='estimated function')\n",
    "    plt.scatter(X, y, color='black', alpha=.5, label='random sample')\n",
    "    plt.ylim(np.min(f)-4*sigma, np.max(f)+4*sigma)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('lambda: %.3g' % lmbda)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('estimated m(x)')\n",
    "    plt.show()\n",
    "    sleep(sleeptime)\n",
    "    \n",
    "def true_fn(x):\n",
    "    return 3*x**2\n",
    "\n",
    "def run_simulation(kernel, lmbdas, show_bias_variance=True):\n",
    "    min_x, max_x = -1, 1\n",
    "    x = np.linspace(min_x, max_x, 100)\n",
    "    f = true_fn(x)\n",
    "    sigma = .25\n",
    "    estimates = []\n",
    "    trials = 500\n",
    "\n",
    "    for lmbda in lmbdas:\n",
    "        estimates_lambda = []\n",
    "        for i in np.arange(trials):\n",
    "            X = np.sort(np.random.uniform(low=min_x, high=max_x, size=50))\n",
    "            fX = true_fn(X)\n",
    "            y = fX + sigma*np.random.normal(size=len(X))\n",
    "            fhat = mercer_kernel_regress(kernel, X, y, x, lmbda=lmbda)\n",
    "            if i % 50 == 0:\n",
    "                plot_estimate(x, f, fhat, X, y, sigma, lmbda)\n",
    "            estimates_lambda.append(fhat)\n",
    "        estimates.append(estimates_lambda)\n",
    "\n",
    "    if show_bias_variance == False:\n",
    "        return\n",
    "    \n",
    "    fhat = np.array(estimates)\n",
    "    sq_bias = np.zeros(len(lmbdas))\n",
    "    variance = np.zeros(len(lmbdas))\n",
    "\n",
    "    for i in np.arange(len(lmbdas)):\n",
    "        sq_bias[i] = np.mean((np.mean(fhat[i], axis=0) - f)**2)\n",
    "        variance[i] = np.mean(np.var(fhat[i], axis=0))\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(lmbdas, sq_bias, label='squared bias', linewidth=2)\n",
    "    plt.plot(lmbdas, variance, label='variance', linewidth=2)\n",
    "    plt.plot(lmbdas, sq_bias + variance, label='risk')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job is to implement Mercer kernel regression and run it on two \n",
    "different kernel functions. The two kernels could simply be the Gaussian kernel\n",
    "with two different bandwidths, or you might experiment with other kernels.\n",
    "\n",
    "The function `mercer_kernel_regress` takes a kernel, training data `X` and `y`, an array of values `x` to evaluate the function on, and a regularization parameter. You'll use your derivation above to \n",
    "determine the coefficients $\\alpha$. For some clues and suggestions on how to do the \n",
    "implementation, see our demo code for smoothing kernels. You need to do something very similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mercer_kernel_regress(kernel, X, y, x, lmbda):  \n",
    "    # your implementation here\n",
    "    _\n",
    "\n",
    "def kernel1(x,y):\n",
    "    # your implementation here\n",
    "    _\n",
    "\n",
    "    \n",
    "def kernel2(x,y):\n",
    "    # your implementation here\n",
    "    _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.3 Run two simulations and select regularization parameters\n",
    "\n",
    "Finally, using our starter code and your own implementation above, run two simulations, one \n",
    "using `kernel1` and the other using `kernel2`. After each simulation, select a regularization level from the bias-variance tradeoff, and then run a final simulation with that regularization level. In the following \n",
    "starter code, you only need to specify the sequence of regularization parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbdas = # define your sequence of lambdas\n",
    "run_simulation(kernel1, lmbdas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_hat = # set the optimal lambda\n",
    "run_simulation(kernel1, [lambda_hat], show_bias_variance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbdas = # define your sequence of lambdas\n",
    "run_simulation(kernel2, lmbdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Double descent! (15 points)\n",
    "\n",
    "<a href=\"https://skitheworld.com/2018/12/wurtele-twins-appointed-order-canada/\"><img src=\"https://raw.githubusercontent.com/YData123/sds365-fa22/main/assignments/assn1/double.jpg\" width=\"200\" align=\"left\" style=\"margin:10px 30px 10px 0px\"></a>\n",
    "\n",
    "\n",
    "In this problem you will explore the \"double descent\" phenomenon that was recently \n",
    "discovered as a key principle underlying the performance of deep neural networks.\n",
    "The problem setup is a \"random features\" version of a 2-layer neural network. The weights in the first layer are random and fixed, and the weights in the second layer are estimated from data. As we increase the number of neurons in the hidden layer, the dimension $p$ of model increases. It's helpful to define the ratio $\\gamma = p/n$ of variables to sample points. If $\\gamma < 1$ then we want to use the OLS estimator, and if $\\gamma > 1$ we want to use the minimum norm estimator. \n",
    "<br>\n",
    "\n",
    "Your mission (should you choose to accept it), is\n",
    "\n",
    "1. Implement a function `OLS_or_minimum_norm` that computes the least squares solution when $\\gamma < 1$, and the minimum norm solution when $\\gamma > 1$. (When $\\gamma=1$ the estimator does not, in general, exist.)\n",
    "1. Run the main code we give you to average over many trials, and to compute and plot the estimated risk for a range of values of $\\gamma$. \n",
    "1. Next, extend the starter code so that you compute (estimates of) the squared-bias and variance of the models. To do this, note that you'll need access to the true regression function, which is provided. You may want to refer to the demo code for smoothing kernels as an example.\n",
    "1. Using your new code, extend the plotting function we provide so that you plot \n",
    "the squared-bias, variance, and risk together on the same plot. \n",
    "1. Finally, comment on the results, describing why it might make sense that the squared bias, variance, and risk have the given shapes that they do.\n",
    "\n",
    "\n",
    "By doing this exercise you will solidify your understanding of the meaning of bias and variance, and also gain a better understanding of the \"double descent\" phenomenon for overparameterized neural networks, \n",
    "and their striking resistance to overfitting.\n",
    "\n",
    "We're available in OH to help with any issues you run into!\n",
    "\n",
    "If you have any interest in background reading on this topic (not expected or required), we recommend Hastie et al., [\"Surprises in high-dimensional ridgeless least squares regression\"](https://www.stat.cmu.edu/~ryantibs/papers/ridgeless.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1\n",
    "\n",
    "Implement the function `OLS_or_minimum_norm` that computes the OLS solution for $\\gamma < 1$, and the minimum norm solution for $\\gamma > 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_or_minimum_norm(X, y):\n",
    "    ## Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plotting function we provide. No need to change this, although you can if you'd like.\n",
    "\n",
    "def plot_double_descent_risk(gammas, risk, sigma):\n",
    "    gammas = np.round(gammas, 2)\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    tick_pos = np.zeros(len(gammas))\n",
    "    for i in np.arange(len(gammas)):\n",
    "        if gammas[i] <= 1:\n",
    "            tick_pos[i] = gammas[i] * 10\n",
    "        else:\n",
    "            tick_pos[i] = gammas[i] + 9\n",
    "    ax.axvline(x=tick_pos[np.array(gammas)==1][0], linestyle='dashed', color='gray')\n",
    "    ax.axhline(y=sigma**2, linestyle='dashed', color='gray')\n",
    "    ax.scatter(tick_pos, risk, color='salmon')\n",
    "    ax.plot(tick_pos, risk, color='gray', linewidth=.5)\n",
    "\n",
    "    tickgam = [gam for gam in gammas if (gam > .05 and gam <= .9) or gam >= 2 or gam == 1]\n",
    "    ticks = [tick_pos[j] for j in np.arange(len(tick_pos)) if gammas[j] in tickgam]\n",
    "    ax.xaxis.set_ticks(ticks)\n",
    "    ax.xaxis.set_ticklabels(tickgam)\n",
    "\n",
    "    plt.xlabel(r'$\\gamma = \\frac{p}{n}$', fontsize=18)\n",
    "    _ = plt.ylabel('Risk', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setup \n",
    "\n",
    "The following cell sets up our data. The inputs $X$ are \n",
    "random Gaussian vectors of dimension $d=10$. Then, we map these \n",
    "using a neural network with fixed, Gaussian weights, to get random features\n",
    "corresponding to $p^* = 150$ hidden neurons. The second layer \n",
    "coefficients are $\\beta^* \\in {\\mathbb R}^{p^*}$, which are fixed. \n",
    "This defines the true model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just execute this cell, after you define the function above.\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "sigma = 1\n",
    "d = 10\n",
    "p_star = 150\n",
    "signal_size = 5\n",
    "\n",
    "W_star = (1/np.sqrt(d)) * np.random.randn(d, p_star)\n",
    "beta_star = np.arange(p_star)\n",
    "beta_star = signal_size * beta_star / np.sqrt(np.sum(beta_star**2))\n",
    "\n",
    "N = 10000\n",
    "X = np.random.randn(N, d)\n",
    "\n",
    "# f_star is the true regression function, for computing the squared bias\n",
    "f_star = np.dot(np.tanh(np.dot(X, W_star)), beta_star)\n",
    "noise = sigma * np.random.randn(N)\n",
    "y = f_star + noise\n",
    "yf = np.concatenate((y.reshape(N,1), f_star.reshape(N,1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a sequence of models for different values of $\\gamma$\n",
    "\n",
    "Next, we train a sequence of models for different values of $\\gamma$, always \n",
    "fixing the sample size at $n=200$, but varying the dimension $p = \\gamma n$. \n",
    "When $p < p^*$ we just take the first $p$ features in the true model. \n",
    "When $p > p^*$ we add $p-p^*$ neurons to the hidden layer, with their \n",
    "own random weights.\n",
    "\n",
    "In the code below, we loop over the different values of $\\gamma$, \n",
    "and for each $\\gamma$ we run $100$ trials, each time generating \n",
    "a new training set of size $n=200$. The model (either OLS or minimum norm) is then computed, the MSE is computed, and finally the risk is estimated by averaging over all $100$ trials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trials = 100\n",
    "n = 200\n",
    "\n",
    "gammas = list(np.arange(.1, 1, .1)) + [.92, .94, 1, 1.1, 1.2, 1.4, 1.6] + list(np.arange(2, 11, 1))\n",
    "gammas = [.01, .05] + gammas\n",
    "risk = []\n",
    "for gamma in gammas:\n",
    "    err = []\n",
    "    p = int(n * gamma)\n",
    "    if gamma == 1:\n",
    "        risk.append(np.inf)\n",
    "        continue\n",
    "    W = (1/np.sqrt(d)) * np.random.randn(d, p)\n",
    "    W[:,:min(p, p_star)] = W_star[:,:min(p, p_star)]\n",
    "    for i in np.arange(trials):\n",
    "        X_train, X_test, yf_train, yf_test = train_test_split(X, yf, train_size=n, test_size=1000)\n",
    "        H_train = np.tanh(np.dot(X_train, W))\n",
    "        H_test = np.tanh(np.dot(X_test, W))\n",
    "        beta_hat = OLS_or_minimum_norm(H_train, yf_train[:,0])\n",
    "        yhat_test = H_test @ beta_hat \n",
    "        err.append(np.mean((yhat_test - yf_test[:,0])**2))\n",
    "    print('gamma=%.2f  p=%d  n=%d  risk=%.3f' % (gamma, p, n, np.mean(err)))\n",
    "    risk.append(np.mean(err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the risk\n",
    "\n",
    "At this point, you can plot the risk by just evaluating the cell below. \n",
    "This should reveal the \"double descent\" behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just evaluate the next line \n",
    "plot_double_descent_risk(gammas, risk, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.2\n",
    "\n",
    "Comment on the results. Explain why the risk plot does or does not make sense \n",
    "in each regime: The underparameterized regime $\\gamma < 1$, and the overparameterized \n",
    "regime $\\gamma > 1$. Is the curve \"U-shaped\" in the underparameterized regime? Why or why not?\n",
    "What about in the overparameterized regime? You will be able to give better answers to these questions when you estimate the bias and variance below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your markdown here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.3\n",
    "\n",
    "Now, modify the above code so that you can estimate both the squared bias and the \n",
    "variance of the estimator. Before you do this, you may want to revisit the kernel smoothing demo from class, where we computed the squared bias, variance, and risk. You'll need the true function, which is provided in the variable `yf`.  You should not have to write a lot of code, but can compute the bias and variance after you store the predicted values on the test data for each trial.\n",
    "\n",
    "Plot the results, by plotting both the squared bias, the variance, and the risk for the sequence of gammas. To do this you will have to modify the plotting function appropriately, but this again involves minimal changes. When you obtain your final plot, comment \n",
    "on the shape of the bias and variance curves, as above for Problem 4.2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and markdown here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
