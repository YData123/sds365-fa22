{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Machine Learning: Assignment 2\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 2 is due Wednesday, October 12 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on Canvas).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a pdf file on Gradescope, and as a notebook (.ipynb) on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:\n",
    "\n",
    "Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "Under \"Download as\", select \"HTML (.html)\"\n",
    "After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "\n",
    " * Convolutional neural networks\n",
    " * Gaussian processes\n",
    "\n",
    "This assignment will also help to solidify your Python and Jupyter notebook skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Brain food (15 points)\n",
    "\n",
    "This problem gives you some experience with convolutional neural networks for image classification using [TensorFlow](https://en.wikipedia.org/wiki/TensorFlow).  \n",
    "\n",
    "The classification task is to discriminate real optical images of brain activity in mice from \n",
    "fake images that were constructed using a [generative adversarial network (GAN)](https://en.wikipedia.org/wiki/Generative_adversarial_network).\n",
    "A paper on the underlying imaging technologies developed by Yale researchers is [here](https://www.nature.com/articles/s41592-020-00984-6).\n",
    "\n",
    "For this problem we'll step you through the following steps:\n",
    "* Downloading the data\n",
    "* Loading the data\n",
    "* Displaying some sample images\n",
    "\n",
    "After this, your task will be design, build, and train a CNN for the classification task, and to comment on the structure and the performance of your best model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Downloading the data\n",
    "\n",
    "The data are contained in a group of compressed files on AWS. There are 10 files of real images, \n",
    "and 10 files of fake images; each file is roughly 100 MB in size; so the entire dataset is about 2 GB.\n",
    "You should download the data to the computer you are running on, and place the in a folder named \"data\".\n",
    "\n",
    "*Important note:* If you do not have enough space to download all of the data, just download what you can;\n",
    "there will be no penalty for running on less data. If you want assistance running in Google Colab, please let us know.\n",
    "\n",
    "Here are URLs to access the 20 data files:\n",
    "\n",
    "\n",
    "https://sds365.s3.amazonaws.com/calcium/real_0.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/real_1.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/real_2.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/real_3.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/real_4.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/real_5.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/real_6.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/real_7.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/real_8.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/real_9.gz <br>\n",
    "\n",
    "\n",
    "https://sds365.s3.amazonaws.com/calcium/fake_0.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/fake_1.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/fake_2.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/fake_3.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/fake_4.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/fake_5.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/fake_6.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/fake_7.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/fake_8.gz <br>\n",
    "https://sds365.s3.amazonaws.com/calcium/fake_9.gz <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import some Python packages from TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import numpy as np\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some helper functions for reading the data and plotting images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(imgs, title):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(9):\n",
    "        plt.subplot(3,3,i+1)\n",
    "        plt.imshow(imgs[i], cmap='rainbow')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    \n",
    "def read_gz(filedir, shape=[-1,128,128]):\n",
    "    print('reading %s' % filedir)\n",
    "    with gzip.open(filedir, 'rb') as f:\n",
    "        content = f.read()\n",
    "    imgs = np.frombuffer(content, dtype='float32').reshape(shape)\n",
    "    return imgs\n",
    "\n",
    "def load_data(pieces):\n",
    "    data = []\n",
    "    label = []\n",
    "    print('Loading data:\\n-------------')\n",
    "    for i in pieces:\n",
    "        real_img = read_gz('data/real_{:d}.gz'.format(i), shape=[-1,128,128, 1])\n",
    "        fake_img = read_gz('data/fake_{:d}.gz'.format(i), shape=[-1,128,128, 1])\n",
    "        real_label = np.zeros((real_img.shape[0],1))\n",
    "        fake_label = np.zeros((fake_img.shape[0],1))\n",
    "        real_label[:,0] = 0\n",
    "        fake_label[:,0] = 1\n",
    "        data.append(real_img)\n",
    "        data.append(fake_img)\n",
    "        label.append(real_label)\n",
    "        label.append(fake_label)\n",
    "    print()\n",
    "    data_all = np.concatenate(data, axis=0)\n",
    "    label_all = np.concatenate(label, axis=0)\n",
    "    return data_all, label_all\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data \n",
    "\n",
    "Let's look at some images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_images are original data, fake_images are synthetic data generated using a GAN model\n",
    "real_images = read_gz('data/real_0.gz')\n",
    "fake_images = read_gz('data/fake_0.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the images is 128x128 pixels, and there are 2048 images in each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images.shape, fake_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying some sample images\n",
    "\n",
    "Now we'll display some real and fake images. Can you spot any differences between the two. Do you think that you could learn to tell them apart? Just think about this, you do not need to write out an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(real_images, 'real images')\n",
    "plot_images(fake_images, 'fake images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Building a CNN model\n",
    "\n",
    "Your model should be trained on six of the data files (3 real, and 3 fake, about 12,000 images total) and is tested on six of the data files. We begin by loading in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = load_data([0,1,2])\n",
    "test_images, test_labels = load_data([7,8,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, construct a convolutional neural network that contains at least four layers: A convolutional layer, a max pooling layer, a flattened layer, and a final dense layer with two terminal neurons.\n",
    "The kernel size and number are among the choices you can make. Similarly, you can \n",
    "choose your own pooling size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compile and train the model. Here you can choose how many epochs you use \n",
    "to train the model, where each epoch scans through the \n",
    "data in a random order, processing a batch of images in each stochastic gradient descent step. Your code should\n",
    "train the model with the Adam optimizer and the binary cross-entropy loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, evaluate the model on the test data (the last three segments of images: 7,8,9). Your code should evaluate the model performance on test images and print out the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Discuss your model\n",
    "\n",
    "Before moving into the discussion, consider improving your model in various ways:\n",
    "\n",
    "* Train on more data (but always test on the same test set)\n",
    "* Add more convolutional layers \n",
    "* Add more dense layers, using an appropriate activation function\n",
    "* Regularize using a dropout layer\n",
    "\n",
    "For the final model that you are satisfied with, discuss how and why you built this model. For example, you can:\n",
    "\n",
    "* Give a diagram showing the sequence of layers used;\n",
    "* Explain your rationale for using each of the layers;\n",
    "* Comment on the number of parameters used by the model, and which layers have the most parameters;\n",
    "* Describe your findings on number of epochs and training data size;\n",
    "* Comment on the models you experimented with but did not include and how your modifications changed the test accuracy.\n",
    "\n",
    "Note: Use suggestions as a guideline. When we evaluate your notebook, we will look for descriptions of your models that show understanding of how they work and why you chose a given architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your discussions and perhaps some plots here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: It's not a bug, it's a feature! (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will [\"open the black box\"](https://news.yale.edu/2018/12/10/why-take-ydata-because-data-science-shouldnt-be-black-box) and inspect the filters and feature maps learned by a convolutional neural network trained to classify handwritten digits, using the MNIST database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Visualizing the filters\n",
    "\n",
    "To begin, we load the dataset with 60000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_binary = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize our convolutional neural network similar to the network we used for Problem 1 except that we now have a few more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", name='conv1'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", name='conv2'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 1\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train_binary, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test_binary, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained and tested the model, let's look at the filters learned in the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_conv1 = model.get_layer(name='conv1').get_weights()[0]\n",
    "\n",
    "fig, axs = plt.subplots(4, 8)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(8):\n",
    "        f = filters_conv1[:, :, 0, 8*i+j]\n",
    "        axs[i, j].imshow(f[:, :], cmap='gray')\n",
    "        axs[i, j].axis('off')\n",
    "        axs[i, j].set_title(8*i+j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe what you see. Do (some of) the learned filters make sense to you?\n",
    "\n",
    "Hint: Many filters have been designed and widely applied in image processing. [Here](http://www.theobjects.com/dragonfly/dfhelp/3-5/Content/05_Image%20Processing/Edge%20Detection%20Filters.htm) are some examples of edge detection filters and their effect on the image. You can find the details about each filter by clicking the links at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Markdown Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing the feature maps\n",
    "\n",
    "We can also look at the corresponding feature map for each filter. There are 32 kernels at the first convolutional layer, so there are 32 feature maps for each sample. feature_map_conv1 is a 4D matrix where the first dimension is the index of the sample and the last dimension is the index of the correpsonding filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_layer_model = keras.Model(inputs=model.input, outputs=model.get_layer('conv1').output)\n",
    "feature_map_conv1 = conv1_layer_model(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly draw 16 samples for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = random.sample(range(1, len(x_test)), 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose two filters among all 32 filters from 2.1, and visualize their feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_n1 = #\n",
    "filter_n2 = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to modify the next code cells, just run the four cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(filters_conv1[:, :, 0, filter_n1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 8)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ix=0\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axs[i, 2*j].imshow(x_test[sample_index[4*i+j], :, :, 0], cmap='gray')\n",
    "        axs[i, 2*j].axis('off')\n",
    "        axs[i, 2*j+1].imshow(feature_map_conv1[sample_index[4*i+j], :, :, filter_n1], cmap='gray')\n",
    "        axs[i, 2*j+1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(filters_conv1[:, :, 0, filter_n2], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 8)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ix=0\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axs[i, 2*j].imshow(x_test[sample_index[4*i+j], :, :, 0], cmap='gray')\n",
    "        axs[i, 2*j].axis('off')\n",
    "        axs[i, 2*j+1].imshow(feature_map_conv1[sample_index[4*i+j], :, :, filter_n2], cmap='gray')\n",
    "        axs[i, 2*j+1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on what you see in the feature maps.\n",
    "* How do they correspond to the original images?\n",
    "* How do they correspond to the filters?\n",
    "* Why might the feature maps be helpful for classifying digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Fitting a logistic regression model on feature maps\n",
    "\n",
    "The features of the images are further summarized after the second convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_layer_model = keras.Model(inputs=model.input, outputs=model.get_layer('conv2').output)\n",
    "feature_map_conv2 = conv2_layer_model(x_test)\n",
    "\n",
    "fig, axs = plt.subplots(4, 8)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ix=0\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axs[i, 2*j].imshow(x_test[sample_index[4*i+j], :, :, 0], cmap='gray')\n",
    "        axs[i, 2*j].axis('off')\n",
    "        axs[i, 2*j+1].imshow(feature_map_conv2[sample_index[4*i+j], :, :, 0], cmap='gray')\n",
    "        axs[i, 2*j+1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and test a logistic regression model to classify two digits of your choice (i.e. a binary classification) using the features maps at the second convolutional layer as the input. You may use logistic regression functions such as [LogisticRegression in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Use 80% of the data for training and 20% for test.\n",
    "\n",
    "* How many features are there in your input X? Show the derivation of this number based on the architecture of the convolutional neural network.\n",
    "\n",
    "* How is your logistic regression model related to the fully connected layer and softmax layer in the convolutional neural network?\n",
    "\n",
    "* What is the accuracy of your model? Is this expected, or surprising? \n",
    "\n",
    "* Comment on any other aspects of your findings that are interesting to you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lr = np.reshape(feature_map_conv2,(np.shape(feature_map_conv2)[0],-1))\n",
    "y_lr = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: All that glitters (20 points)\n",
    "\n",
    "In this problem you will use Gaussian process regression to model the trends in gold medal performances of selected events in the summer Olympics. The objectives of this problem are for you to:\n",
    "\n",
    "* Gain experience with Gaussian processes, to better understand how they work\n",
    "* Explore how posterior inference depends on the properties of the prior mean and kernel\n",
    "* Use Bayesian inference to identify unusual events\n",
    "* Practice making your Python code modular and reusable\n",
    "\n",
    "For this problem, the only starter code we provide is to read in the data and extract \n",
    "one event. You may write any GP code that you choose to, but please do not use any \n",
    "package for Gaussian processes; your code should be \"np-complete\" (using only \n",
    "basic `numpy` methods). You are encouraged to start from the [GP demo code](https://ydata123.org/sp22/interml/calendar.html) used in class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ran the GP demo code from class on the marathon data, it generated the following plot:\n",
    "<img src=\"https://github.com/YData123/sds365-fa22/raw/main/assignments/assn2/marathon.jpg\" width=\"600\">\n",
    "\n",
    "Note several properties of this plot:\n",
    "* It shows the Bayesian confidence of the regression, as a shaded area. This is a 95% confidence band because it has width given by $\\pm 2 \\sqrt{V}$, where $V$ is the estimated variance. The variance increases at the right side, for future years.\n",
    "\n",
    "* The gold medal time for the 1904 marathon is outside of this confidence band. In fact, \n",
    "the 1904 marathon was an [unusual event](https://www.smithsonianmag.com/history/the-1904-olympic-marathon-may-have-been-the-strangest-ever-14910747/), and this is apparent from the model. \n",
    "\n",
    "* The plot shows the posterior mean, and also shows one random sample from the posterior distribution.\n",
    "\n",
    "Your task in this problem is generate such a plot for six different Olympic events by writing a function\n",
    "\n",
    "`def gp_olympic_event(year, result, kernel, mean, noise, event_name):\n",
    "    ...`\n",
    "    \n",
    " where the input variables are the following:\n",
    " \n",
    "* `year`: a numpy array of years (integers)\n",
    "* `result`: a numpy array of numerical results, for the gold medal performances in that event\n",
    "* `kernel`: a kernel function \n",
    "* `mean`: a mean function \n",
    "* `noise`: a single float for the variance of the noise, $\\sigma^2$\n",
    "* `event_name`: a string used to label the y-axis, for example \"marathon min/mile (men's event)\"\n",
    " \n",
    "Your function should compute the Gaussian process regression, and then display the resulting plot, analogous to the plot above for the men's marathon event.\n",
    "\n",
    "You will then process **six** of the events, three men's events and three women's events, and call your function to generate the corresponding six plots.\n",
    "\n",
    "For each event, you should create a markdown cell that describes the resulting model. Comment on such things as:\n",
    "\n",
    "* How you chose the kernel, mean, and noise.\n",
    "* Why the plot does or doesn't look satisfactory to you\n",
    "* If there are any events such as the 1904 marathon that are notable.\n",
    "* What happens to the posterior mean (for example during WWII) if there are gaps in the data\n",
    "\n",
    "Use your best judgement to describe your findings; post questions to EdD if things are unclear. And have fun!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "In the remainder of this problem description, we recall how we processed the marathon data, as an example. The following cell reads in the data and displays the collection of events that are included in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dat = pd.read_csv('https://raw.githubusercontent.com/YData123/sds365-sp22/main/demos/gaussian_processes/olympic_results.csv')\n",
    "events = set(np.array(dat['Event']))\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then process the time to compute the minutes per mile (without checking that the race was actually 26.2 miles!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marathon = dat[dat['Event'] == 'Marathon Men']\n",
    "marathon = marathon[marathon['Medal']=='G']\n",
    "marathon = marathon.sort_values('Year')\n",
    "time = np.array(marathon['Result'])\n",
    "mpm = []\n",
    "for tm in time:\n",
    "    t = np.array(tm.split(':'), dtype=float)\n",
    "    minutes_per_mile = (t[0]*60*60 + t[1]*60 + t[2])/(60*26.2)\n",
    "    mpm.append(minutes_per_mile)\n",
    "    \n",
    "marathon['Minutes per Mile'] = np.round(mpm,2)\n",
    "marathon = marathon.drop(columns=['Gender', 'Event'], axis=1)\n",
    "marathon.reset_index(drop=True, inplace=True)\n",
    "year = np.array(marathon['Year'])\n",
    "result = np.array(marathon['Minutes per Mile'])\n",
    "marathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your code and markdown following this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4a7aa81d2787d79eab8f6fb8f7b5343089747e772b346faf833d5b60a4f70bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
